{
 "metadata": {
  "kernelspec": {
   "name": "venv",
   "display_name": "venv",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "PREPROCESS = True # Do a fresh preprocess\n",
    "MAKE_NEW_EMBEDDING = True # If False, the stored one will be loaded\n",
    "# EMB_MAX_WORDS = None\n",
    "RANDOM_SEED = 456\n",
    "SAVE_TRAINED_MODEL = True\n",
    "\n",
    "PREPROCESS_INPUT = './data/training.1600000.processed.noemoticon.csv'\n",
    "PREPROCESS_OUTPUT = './data/preprocessed.csv'\n",
    "GLOVE_FILE = './data/glove.6B.50d.txt'\n",
    "EMB_PKL = './models/emb_layer.pkl'\n",
    "MODEL_PKL = './models/model.pkl'\n",
    "MODEL_TYPE = 'sequential'\n",
    "model_path = 'models/trained_'+ MODEL_TYPE\n",
    "TWEETS_TO_EXPLAIN = [111, 156, 933487, 933565]\n",
    "\n",
    "new_model = False"
   ]
  },
  {
   "source": [
    "### Preprocess data and store it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre import preprocess\n",
    "\n",
    "if PREPROCESS:\n",
    "    preprocess(i=PREPROCESS_INPUT, o=PREPROCESS_OUTPUT, slice=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(PREPROCESS_OUTPUT)\n",
    "X = data['tweet']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, 2)\n"
   ]
  },
  {
   "source": [
    "### Create Empty model\n",
    "%autoreload 2\n",
    "\n",
    "from TextClassifierModel import new_classifier, load_classifier, save_classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if new_model:\n",
    "    model_type = \"recurrent\"\n",
    "    text_classifier = new_classifier(glove_file = GLOVE_FILE, data=data, model_type=model_type)\n",
    "    save_classifier(text_classifier, 'models/untrained')\n",
    "else:\n",
    "    text_classifier = load_classifier(model_path=model_path)\n",
    "print(text_classifier.model.summary())\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 93,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 100, 50)           140400    \n_________________________________________________________________\nflatten (Flatten)            (None, 5000)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 5000)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               1280256   \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_2 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 130       \n=================================================================\nTotal params: 1,461,938\nTrainable params: 1,321,538\nNon-trainable params: 140,400\n_________________________________________________________________\nNone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from evaluation import plot_history\n",
    "\n",
    "history = text_classifier.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=60, epochs=30, verbose=1)\n",
    "plot_history(history)\n"
   ]
  },
  {
   "source": [
    "### Save trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from TextClassifierModel import save_classifier\n",
    "save_classifier(text_classifier, model_path)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate production (forbidden during tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_model\n",
    "\n",
    "evaluate_model(text_classifier, X_test, y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unprocessed data for readability\n",
    "from pre import load_unprocessed\n",
    "df_orig = load_unprocessed(PREPROCESS_INPUT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 663.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fetch subset of training tweets\n",
    "%autoreload 2\n",
    "from pre import split_and_preprocess\n",
    "indices = [111, 156, 933487, 933565]\n",
    "\n",
    "explain_tweets_orig, explain_tweets_prep = split_and_preprocess(df_orig, TWEETS_TO_EXPLAIN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean_KL_divergence 0.0024789303535317424\n",
      "score 1.0\n",
      "mean_KL_divergence 0.0009783858683550606\n",
      "score 0.9568860590655804\n",
      "mean_KL_divergence 0.0014899328141211635\n",
      "score 0.7429565291074613\n",
      "mean_KL_divergence 0.00214878942366003\n",
      "score 1.0\n"
     ]
    }
   ],
   "source": [
    "from explainability import explain_and_save\n",
    "explain_and_save(explain_tweets_orig, explain_tweets_prep, TWEETS_TO_EXPLAIN, text_classifier,  MODEL_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n\n        \n    \n        \n        \n    \n        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n            <b>\n    \n        y=1\n    \n</b>\n\n    \n    (probability <b>0.585</b>, score <b>0.344</b>)\n\ntop features\n        </p>\n    \n    <table class=\"eli5-weights\"\n           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n        <thead>\n        <tr style=\"border: none;\">\n            \n                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n                    Contribution<sup>?</sup>\n                </th>\n            \n            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n            \n        </tr>\n        </thead>\n        <tbody>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.239\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        Highlighted in text (sum)\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.74%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.105\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        &lt;BIAS&gt;\n    </td>\n    \n</tr>\n        \n        \n\n        \n        \n\n        </tbody>\n    </table>\n\n    \n\n\n\n    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n        <span style=\"opacity: 0.80\">[&quot;knew&quot;, &quot;</span><span style=\"background-color: hsl(120, 100.00%, 84.19%); opacity: 0.85\" title=\"0.029\">text</span><span style=\"opacity: 0.80\">&quot;, &quot;</span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.109\">could</span><span style=\"opacity: 0.80\">&quot;, &quot;</span><span style=\"background-color: hsl(120, 100.00%, 85.35%); opacity: 0.85\" title=\"0.026\">make</span><span style=\"opacity: 0.80\">&quot;, &quot;</span><span style=\"background-color: hsl(120, 100.00%, 88.44%); opacity: 0.83\" title=\"0.018\">someone</span><span style=\"opacity: 0.80\">&quot;, &quot;</span><span style=\"background-color: hsl(0, 100.00%, 95.63%); opacity: 0.81\" title=\"-0.005\">so</span><span style=\"opacity: 0.80\">&quot;, &quot;</span><span style=\"background-color: hsl(0, 100.00%, 84.45%); opacity: 0.85\" title=\"-0.028\">happy</span><span style=\"opacity: 0.80\">&quot;]</span>\n    </p>\n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "#Show prediction for single tweet:\n",
    "tweet_idx = TWEETS_TO_EXPLAIN[3]\n",
    "te = TextExplainer(random_state=42)\n",
    "te.fit(explain_tweets_prep[tweet_idx], text_classifier.predict_proba)\n",
    "te.show_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from visualize_embeddings import  display_pca_scatter_plot\n",
    "display_pca_scatter_plot(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}